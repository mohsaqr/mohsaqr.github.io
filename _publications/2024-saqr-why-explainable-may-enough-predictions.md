---
layout: publication
title: "Why explainable AI may not be enough: predictions and mispredictions in decision making in education"
authors:
  - family: "Saqr"
    given: "Mohammed"
  - family: "López-Pernas"
    given: "Sonsoles"
year: 2024
journal: "Smart Learning Environments"
volume: "11"
issue: "1"
pages: "52"
doi: "10.1186/s40561-024-00343-4"
affiliations: "School of Computing, University of Eastern Finland, Joensuu Campus Yliopistokatu 2, Joensuu, FI-80100, Finland"
doc_type: "Article"
abbr: "SLE"
selected: true
source: "both"
dimensions_url: "https://app.dimensions.ai/details/publication/pub.1182460908"
scopus_eid: "2-s2.0-85209580632"
times_cited: 16
---

In learning analytics and in education at large, AI explanations are always computed from aggregate data of all the students to offer the “average” picture. Whereas the average may work for most students, it does not reflect or capture the individual differences or the variability among students. Therefore, instance-level predictions—where explanations for each particular student are presented according to their own data—may help understand how and why predictions were estimated and how a student or teacher may act or make decisions. This study aims to examine the utility of individualized instance-level AI, its value in informing decision-making, and—more importantly—how they can be used to offer personalized feedback. Furthermore, the study examines mispredictions, their explanations and how they offer explanations or affect decision making. Using data from a full course with 126 students, five ML algorithms were implemented with explanatory mechanisms, compared and the best performing algorithm (Random Forest) was therefore selected. The results show that AI explanations, while useful, cannot achieve their full potential without a nuanced human involvement (i.e., hybrid human AI collaboration). Instance-level explainability may allow us to understand individual algorithmic decisions but may not very helpful for personalization or individualized support. In case of mispredictions, the explanations show that algorithms decide based on the “wrong predictors” which underscores the fact that a full data-driven approach cannot be fully trusted with generating plausible recommendations completely on its own and may require human assistance. © The Author(s) 2024.
