---
layout: publication
title: "Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and Individual Predictions"
authors:
  - family: "Saqr"
    given: "Mohammed"
  - family: "López-Pernas"
    given: "Sonsoles"
year: 2026
journal: "Advanced Learning Analytics Methods"
pages: "165--187"
doi: "10.1007/978-3-031-95365-1_7"
keywords:
  - "DALEX"
  - "Explainable AI"
  - "Learning analytics"
  - "LIME"
  - "Machine learning"
  - "SHAP"
  - "Artificial Intelligence"
affiliations: "University of Eastern Finland, Joensuu, Finland"
doc_type: "Book chapter"
abbr: "Book Chapter"
selected: false
source: "both"
dimensions_url: "https://app.dimensions.ai/details/publication/pub.1194314052"
scopus_eid: "2-s2.0-105025635601"
times_cited: 7
---

In the context of explainable artificial intelligence, global explanations provide aggregate insights about the performance and factors influencing a machine learning model, as we have seen in the previous chapter. However, local explanations are needed to understand the factors influencing a specific decision that affect an individual. For instance, local explanations could help teachers understand why a certain student was flagged as at-risk of dropping out a course, fostering transparency and trust. This chapter highlights the need for local explanations in educational contexts and explores three key techniques: Break Down plots, SHAP (SHapley Additive exPlanations), and LIME (Local Interpretable Model-agnostic Explanations)—implemented. Practical examples demonstrate how these methods address prediction interpretability, identify critical features, and support targeted interventions © 2026 The Editor(s) (if applicable) and The Author(s).
